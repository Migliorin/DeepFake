{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:50:51.46279Z",
     "iopub.status.busy": "2022-04-12T00:50:51.462477Z",
     "iopub.status.idle": "2022-04-12T00:51:00.709957Z",
     "shell.execute_reply": "2022-04-12T00:51:00.708774Z",
     "shell.execute_reply.started": "2022-04-12T00:50:51.462744Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:00.712868Z",
     "iopub.status.busy": "2022-04-12T00:51:00.712568Z",
     "iopub.status.idle": "2022-04-12T00:51:00.717571Z",
     "shell.execute_reply": "2022-04-12T00:51:00.716341Z",
     "shell.execute_reply.started": "2022-04-12T00:51:00.71282Z"
    }
   },
   "outputs": [],
   "source": [
    "SECONDS = None\n",
    "FPS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:00.719857Z",
     "iopub.status.busy": "2022-04-12T00:51:00.719497Z",
     "iopub.status.idle": "2022-04-12T00:51:08.491921Z",
     "shell.execute_reply": "2022-04-12T00:51:08.49074Z",
     "shell.execute_reply.started": "2022-04-12T00:51:00.719792Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, LSTM,Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.layers.convolutional import \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:08.494152Z",
     "iopub.status.busy": "2022-04-12T00:51:08.493685Z",
     "iopub.status.idle": "2022-04-12T00:51:08.508026Z",
     "shell.execute_reply": "2022-04-12T00:51:08.50658Z",
     "shell.execute_reply.started": "2022-04-12T00:51:08.494047Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metadata with video label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:08.512751Z",
     "iopub.status.busy": "2022-04-12T00:51:08.512317Z",
     "iopub.status.idle": "2022-04-12T00:51:08.902679Z",
     "shell.execute_reply": "2022-04-12T00:51:08.901582Z",
     "shell.execute_reply.started": "2022-04-12T00:51:08.512671Z"
    }
   },
   "outputs": [],
   "source": [
    "train_metadata = pd.read_json(\"../input/deepfake-detection-challenge/train_sample_videos/metadata.json\")\n",
    "train_metadata = train_metadata.T\n",
    "train_metadata.reset_index(inplace=True)\n",
    "train_metadata.rename({\"index\":\"name\"},axis=1,inplace=True)\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake/Real videos count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:08.906672Z",
     "iopub.status.busy": "2022-04-12T00:51:08.906248Z",
     "iopub.status.idle": "2022-04-12T00:51:09.191158Z",
     "shell.execute_reply": "2022-04-12T00:51:09.19018Z",
     "shell.execute_reply.started": "2022-04-12T00:51:08.906595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Escolhendo tema grafico\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "# Configurando tamanho grafico\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "bar = sns.countplot(data=train_metadata,x=\"label\",ax=ax)\n",
    "ax.set_title(\"Real and Fake Videos Split\")\n",
    "\n",
    "# Adicionando legendas nas barras\n",
    "for p in bar.patches:\n",
    "    _x = p.get_x() + p.get_width() / 2\n",
    "    _y = p.get_y() + p.get_height() + 4\n",
    "    value = f\"{p.get_height()}\"\n",
    "    ax.text(_x, _y, value, ha=\"center\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:09.193318Z",
     "iopub.status.busy": "2022-04-12T00:51:09.192761Z",
     "iopub.status.idle": "2022-04-12T00:51:24.277273Z",
     "shell.execute_reply": "2022-04-12T00:51:24.276219Z",
     "shell.execute_reply.started": "2022-04-12T00:51:09.193255Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fps_all(train_metadata:pd.DataFrame)->list:\n",
    "    fps = []\n",
    "    for video_name in tqdm(train_metadata.name,total=train_metadata.shape[0]):\n",
    "        path_video = f\"../input/deepfake-detection-challenge/train_sample_videos/{video_name}\"\n",
    "        video = cv2.VideoCapture(path_video)\n",
    "        fps.append(video.get(cv2.CAP_PROP_FPS))\n",
    "        video.release()\n",
    "    return fps\n",
    "\n",
    "train_metadata[\"fps\"] = get_fps_all(train_metadata)\n",
    "FPS = int(train_metadata[\"fps\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:24.279454Z",
     "iopub.status.busy": "2022-04-12T00:51:24.279061Z",
     "iopub.status.idle": "2022-04-12T00:51:24.591184Z",
     "shell.execute_reply": "2022-04-12T00:51:24.589693Z",
     "shell.execute_reply.started": "2022-04-12T00:51:24.279395Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "data = train_metadata[\"fps\"].value_counts()\n",
    "sns.barplot(ax=ax,x=data.index,y=list(data))\n",
    "ax.set_title(\"Frame rate per second\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:24.598357Z",
     "iopub.status.busy": "2022-04-12T00:51:24.597842Z",
     "iopub.status.idle": "2022-04-12T00:51:32.693614Z",
     "shell.execute_reply": "2022-04-12T00:51:32.692622Z",
     "shell.execute_reply.started": "2022-04-12T00:51:24.598282Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDuration(train_metadata:pd.DataFrame)->list:\n",
    "    duration = []\n",
    "    for video_name,fps in tqdm(zip(train_metadata.name,train_metadata[\"fps\"]),total=train_metadata.shape[0]):\n",
    "        path_video = f\"../input/deepfake-detection-challenge/train_sample_videos/{video_name}\"\n",
    "        \n",
    "        vidcapture = cv2.VideoCapture(path_video)\n",
    "        totalNoFrames = vidcapture.get(cv2.CAP_PROP_FRAME_COUNT);\n",
    "        durationInSeconds = round(float(totalNoFrames) / float(fps),4)\n",
    "        duration.append(durationInSeconds)\n",
    "        vidcapture.release()\n",
    "    \n",
    "    return duration\n",
    "\n",
    "train_metadata[\"duration\"] = getDuration(train_metadata)\n",
    "SECONDS = int(train_metadata[\"duration\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:32.695759Z",
     "iopub.status.busy": "2022-04-12T00:51:32.695352Z",
     "iopub.status.idle": "2022-04-12T00:51:33.208788Z",
     "shell.execute_reply": "2022-04-12T00:51:33.207496Z",
     "shell.execute_reply.started": "2022-04-12T00:51:32.695587Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.histplot(ax=ax,data=train_metadata,x=\"duration\")\n",
    "ax.set_title(\"Seconds per video\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Width and Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:33.211828Z",
     "iopub.status.busy": "2022-04-12T00:51:33.211123Z",
     "iopub.status.idle": "2022-04-12T00:51:41.227325Z",
     "shell.execute_reply": "2022-04-12T00:51:41.225938Z",
     "shell.execute_reply.started": "2022-04-12T00:51:33.211549Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_width_height(train_metadata:pd.DataFrame)->tuple:\n",
    "    height = []\n",
    "    width = []\n",
    "\n",
    "    for video_name,fps in tqdm(zip(train_metadata.name,train_metadata[\"fps\"]),total=train_metadata.shape[0]):\n",
    "        path_video = f\"../input/deepfake-detection-challenge/train_sample_videos/{video_name}\"\n",
    "\n",
    "        vidcapture = cv2.VideoCapture(path_video)\n",
    "        height.append(vidcapture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        width.append(vidcapture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        vidcapture.release()\n",
    "\n",
    "    return (width, height)\n",
    "\n",
    "width, height = get_width_height(train_metadata)\n",
    "train_metadata[\"width\"] = width\n",
    "train_metadata[\"height\"] = height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.229466Z",
     "iopub.status.busy": "2022-04-12T00:51:41.229026Z",
     "iopub.status.idle": "2022-04-12T00:51:41.615891Z",
     "shell.execute_reply": "2022-04-12T00:51:41.614608Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.229382Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
    "data = train_metadata[\"width\"].value_counts()\n",
    "bar = sns.barplot(ax=ax[0],x=data.index,y=list(data))\n",
    "for i,pat in enumerate(bar.patches):\n",
    "    ax[0].annotate(f\"{int(pat.get_height())}\",\n",
    "                (i,pat.get_height()))\n",
    "\n",
    "ax[0].set_title(\"Width\")\n",
    "\n",
    "data = train_metadata[\"height\"].value_counts()\n",
    "bar = sns.barplot(ax=ax[1],x=data.index,y=list(data))\n",
    "for i,pat in enumerate(bar.patches):\n",
    "    ax[1].annotate(f\"{int(pat.get_height())}\",\n",
    "                (i,pat.get_height()))\n",
    "    \n",
    "ax[1].set_title(\"Lenght\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.619298Z",
     "iopub.status.busy": "2022-04-12T00:51:41.618305Z",
     "iopub.status.idle": "2022-04-12T00:51:41.628164Z",
     "shell.execute_reply": "2022-04-12T00:51:41.626867Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.6192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Totais: 290\n"
     ]
    }
   ],
   "source": [
    "#FRAMES = SECONDS * FPS\n",
    "FRAMES = 290\n",
    "print(f\"Frames Totais: {FRAMES}\")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.631175Z",
     "iopub.status.busy": "2022-04-12T00:51:41.630341Z",
     "iopub.status.idle": "2022-04-12T00:51:41.649601Z",
     "shell.execute_reply": "2022-04-12T00:51:41.648327Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.63107Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_video(path_video:str,img_index:int,resize:tuple)->list:\n",
    "    frames = []\n",
    "    vidcapture = cv2.VideoCapture(path_video)\n",
    "    index = 0\n",
    "    j = 0\n",
    "    while(vidcapture.isOpened()):\n",
    "        rent, frame = vidcapture.read()\n",
    "        if(not rent):\n",
    "            break\n",
    "        else:\n",
    "            if(len(img_index) - 1 < j):\n",
    "                break\n",
    "            else:\n",
    "                if(index == img_index[j]):\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    frame = cv2.resize(frame,resize)\n",
    "                    frames.append(frame)\n",
    "                    j += 1\n",
    "\n",
    "                index += 1\n",
    "\n",
    "    frames = np.array(frames)\n",
    "    frames = (frames / 255)\n",
    "    frames = frames.reshape(frames.shape[0],frames.shape[1],frames.shape[2],1)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Image Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.652724Z",
     "iopub.status.busy": "2022-04-12T00:51:41.651921Z",
     "iopub.status.idle": "2022-04-12T00:51:41.662985Z",
     "shell.execute_reply": "2022-04-12T00:51:41.661761Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.65265Z"
    }
   },
   "outputs": [],
   "source": [
    "def getImgTensor(n_frames:int)->list:\n",
    "    img_idx = np.round(np.linspace(0, FRAMES, n_frames)).astype(int)\n",
    "    return [img_idx, IMG_SIZE, IMG_SIZE, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.666029Z",
     "iopub.status.busy": "2022-04-12T00:51:41.66524Z",
     "iopub.status.idle": "2022-04-12T00:51:41.679134Z",
     "shell.execute_reply": "2022-04-12T00:51:41.67798Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.665955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  [array([  0,  12,  24,  36,  48,  60,  72,  85,  97, 109, 121, 133, 145,\n",
      "       157, 169, 181, 193, 205, 218, 230, 242, 254, 266, 278, 290]), 224, 224, 1]\n"
     ]
    }
   ],
   "source": [
    "img_tensor = getImgTensor(25)\n",
    "print ('Tensor: ', img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.681866Z",
     "iopub.status.busy": "2022-04-12T00:51:41.681303Z",
     "iopub.status.idle": "2022-04-12T00:51:41.69565Z",
     "shell.execute_reply": "2022-04-12T00:51:41.694534Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.681681Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatchData(train_metadata,batch,batch_size,img_tensor)->tuple:\n",
    "    [len_frames,width,length] = [len(img_tensor[0]),img_tensor[1], img_tensor[2]] # dimensions\n",
    "    img_idx = img_tensor[0] # array index of frames\n",
    "    \n",
    "    batch_data = np.zeros((batch_size,len_frames,width,length,1)) # batch data that will pass forward\n",
    "    batch_labels = np.zeros((batch_size,2)) # batch labels that will pass forward\n",
    "    \n",
    "    #############################################################\n",
    "    # Here is how the batch data is split by callback\n",
    "    if(((batch+1)*batch_size) <= train_metadata.shape[0]):\n",
    "        train_metadata_ = train_metadata.iloc[\n",
    "            batch*batch_size:(batch+1)*batch_size,\n",
    "            :\n",
    "        ]\n",
    "    else:\n",
    "        train_metadata_ = train_metadata.iloc[\n",
    "            batch*batch_size:,\n",
    "            :\n",
    "        ]\n",
    "    \n",
    "    #############################################################\n",
    "    video_posi = 0\n",
    "    name_list = train_metadata_['name'].to_list()\n",
    "    label_list = train_metadata_[\"label\"].to_list()\n",
    "    \n",
    "    for name,label in zip(name_list,label_list):\n",
    "        path_ = f\"../input/deepfake-detection-challenge/train_sample_videos/{name}\"\n",
    "        batch_data[video_posi] = pre_process_video(path_,\n",
    "                                          img_idx,\n",
    "                                          (width,length))\n",
    "        \n",
    "        if(label_list == \"FAKE\"):\n",
    "            batch_labels[video_posi][0] = 1\n",
    "        else:\n",
    "            batch_labels[video_posi][1] = 1\n",
    "            \n",
    "        video_posi += 1\n",
    "            \n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.697837Z",
     "iopub.status.busy": "2022-04-12T00:51:41.697424Z",
     "iopub.status.idle": "2022-04-12T00:51:41.716305Z",
     "shell.execute_reply": "2022-04-12T00:51:41.714965Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.697764Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(train_metadata, batch_size, img_tensor):\n",
    "    while True:\n",
    "        if(len(train_metadata[\"name\"])%batch_size == 0):\n",
    "            num_batches = int(len(train_metadata[\"name\"])/batch_size)\n",
    "        else:\n",
    "            num_batches = int(len(train_metadata[\"name\"])/batch_size) + 1\n",
    "        \n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            yield getBatchData(train_metadata,batch,batch_size,img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.718583Z",
     "iopub.status.busy": "2022-04-12T00:51:41.718282Z",
     "iopub.status.idle": "2022-04-12T00:51:41.73096Z",
     "shell.execute_reply": "2022-04-12T00:51:41.729686Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.718531Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotModelHistory(h):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,4))\n",
    "    ax[0].plot(h.history['loss'])   \n",
    "    ax[0].plot(h.history['val_loss'])\n",
    "    ax[0].legend(['loss','val_loss'])\n",
    "    ax[0].title.set_text(\"Train loss vs Validation loss\")\n",
    "\n",
    "    ax[1].plot(h.history['categorical_accuracy'])   \n",
    "    ax[1].plot(h.history['val_categorical_accuracy'])\n",
    "    ax[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n",
    "    ax[1].title.set_text(\"Train accuracy vs Validation accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Max. Training Accuracy\", max(h.history['categorical_accuracy']))\n",
    "    print(\"Max. Validaiton Accuracy\", max(h.history['val_categorical_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.732992Z",
     "iopub.status.busy": "2022-04-12T00:51:41.732569Z",
     "iopub.status.idle": "2022-04-12T00:51:41.750458Z",
     "shell.execute_reply": "2022-04-12T00:51:41.74923Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.732935Z"
    }
   },
   "outputs": [],
   "source": [
    "def make3dFilter(x):\n",
    "    return tuple([x]*3)\n",
    "\n",
    "def make2dFilter(x):\n",
    "    return tuple([x]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:41.75243Z",
     "iopub.status.busy": "2022-04-12T00:51:41.752111Z",
     "iopub.status.idle": "2022-04-12T00:51:42.868324Z",
     "shell.execute_reply": "2022-04-12T00:51:42.867414Z",
     "shell.execute_reply.started": "2022-04-12T00:51:41.752378Z"
    }
   },
   "outputs": [],
   "source": [
    "def defineModel(img_tensor):\n",
    "    inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "    print(inputShape)\n",
    "    model = Sequential([\n",
    "        Conv3D(16, make3dFilter(5), activation='relu', input_shape=inputShape),\n",
    "        MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(32, make3dFilter(3), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(64, make3dFilter(3), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = defineModel(img_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:42.872665Z",
     "iopub.status.busy": "2022-04-12T00:51:42.872329Z",
     "iopub.status.idle": "2022-04-12T00:51:42.886895Z",
     "shell.execute_reply": "2022-04-12T00:51:42.885858Z",
     "shell.execute_reply.started": "2022-04-12T00:51:42.872621Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_metadata,test_size=0.33,random_state=42,stratify=train_metadata[\"label\"])\n",
    "\n",
    "train_generator = generator(train, BATCH_SIZE, img_tensor)\n",
    "val_generator = generator(test, BATCH_SIZE, img_tensor)\n",
    "\n",
    "if (train.shape[0]%BATCH_SIZE) == 0:\n",
    "    steps_per_epoch = int(train.shape[0]/BATCH_SIZE)\n",
    "else:\n",
    "    steps_per_epoch = (train.shape[0]//BATCH_SIZE) + 1\n",
    "\n",
    "if (test.shape[0]%BATCH_SIZE) == 0:\n",
    "    validation_steps = int(test.shape[0]/BATCH_SIZE)\n",
    "else:\n",
    "    validation_steps = (test.shape[0]//BATCH_SIZE) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:42.888368Z",
     "iopub.status.busy": "2022-04-12T00:51:42.888025Z",
     "iopub.status.idle": "2022-04-12T00:51:42.907207Z",
     "shell.execute_reply": "2022-04-12T00:51:42.906009Z",
     "shell.execute_reply.started": "2022-04-12T00:51:42.88831Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "\n",
    "# callbacks_list = [checkpoint, LR]\n",
    "callbacks_list = [LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-12T00:51:42.908831Z",
     "iopub.status.busy": "2022-04-12T00:51:42.908437Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
